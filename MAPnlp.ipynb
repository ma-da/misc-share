{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy as pac\n",
    "from fuzzywuzzy.process import dedupe as fuzzed\n",
    "\n",
    "#create new dataframe from preprocessed data\n",
    "df = pd.read_csv(\"C:\\\\datasources\\\\fullraw.csv\", sep='|', usecols=['ArticleId', 'tags', 'Title', 'quote', 'description', 'Links', 'cpub', 'PublicationDate', 'note', 'wtkURL', 'Priority', 'Summary_ref_links', 'Note_links'], encoding='utf-8')\n",
    "df.sort_values(by='PublicationDate', ascending=False, inplace=True)\n",
    "\n",
    "#auto-load the SpaCy english lang statistical model\n",
    "nlp = pac.load('en')\n",
    "\n",
    "#testdoc = \"This string tests if Willie Nelson could have been detected in Florida by a Washington D.C. lobbyist for Monsanto in 2003.\"\n",
    "#doc = nlp(testdoc)\n",
    "\n",
    "#combine summary and note text to make corpus for entity recognition\n",
    "df['ents'] = df['description']+ \" \" + df['note'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run entity recognition and create a list of results lists. 10-12min runtime with 9255 paragraphs. slow, but lambda apply strategies take much longer.\n",
    "docs = []\n",
    "for i in df['ents'].astype(str).values:\n",
    "    doc = nlp(i)\n",
    "    redlist = list(set([(e.text, e.label_) for e in doc.ents]))\n",
    "    docs.append(redlist)\n",
    "\n",
    "#check to make sure results list is the same length as dataframe    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean up NER results list with loops and regex, then apply fuzzy deduplication. takes a minute to run on 180K ents in 9255 items\n",
    "#fuzzywuzzy dedupe is finicky about whitespace and special characters\n",
    "ThingsnKinds = []\n",
    "for i in docs:\n",
    "    things = []\n",
    "    kinds = []\n",
    "    for thing,kind in i:\n",
    "        thing.strip()\n",
    "        thing = ' '.join(filter(None,thing.split(' ')))\n",
    "        thing = re.sub(\"[^a-zA-Z0-9 ,'.]+\", \"\", thing)\n",
    "        kind.strip()\n",
    "        kind = ' '.join(filter(None,kind.split(' ')))\n",
    "        kind = re.sub(\"[^a-zA-Z0-9 ,'.]+\", \"\", kind)\n",
    "        if len(thing) > 1:\n",
    "            things.append(thing)\n",
    "        else:\n",
    "            things.append('Nothing')\n",
    "        if len(kind) > 1:  \n",
    "            kinds.append(kind)\n",
    "        else:\n",
    "            kinds.append('ofNote')\n",
    "    magic = dict(zip(things, kinds))\n",
    "    deduped = list(fuzzed(things))\n",
    "    tnk = {k: magic[k] for k in (deduped)}\n",
    "    ThingsnKinds.append(magic)\n",
    "        \n",
    "#check to make sure results list is the same length as dataframe\n",
    "len(ThingsnKinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add recognized entities to dataframe and save to file\n",
    "nod = pd.Series(docs)\n",
    "nodmod = pd.Series(ThingsnKinds)\n",
    "df['NERsMod'] = nodmod.values\n",
    "df['NERs'] = nod.values\n",
    "df[['Title', 'tags', 'quote', 'description', 'Links', 'cpub', 'PublicationDate', 'wtkURL', 'ArticleId', 'Priority', 'Summary_ref_links', 'Note_links', 'NERs', 'NERsMod']].to_csv(\"C:\\\\datasources\\\\totsNER.csv\", sep='|', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use SpaCy to tokenize and lemmatize\n",
    "from collections import Counter as cntr\n",
    "\n",
    "nlp = pac.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "#clean up text for proocessing\n",
    "scrubbed = []\n",
    "df['corpus'] = df['Title']+ \" \" + df['description'].map(str)\n",
    "for i in df['corpus'].astype(str).values:\n",
    "    i.strip()\n",
    "    i = re.sub(\"-\", \" \", i)\n",
    "    i = re.sub(\"'s\", \"\", i)\n",
    "    i = re.sub(\"[^a-zA-Z ]+\", \"\", i)\n",
    "    i = ' '.join(filter(None,i.split(' ')))    \n",
    "    i = i.lower()\n",
    "    doc = nlp(i)\n",
    "    lemmings = []\n",
    "    for e in doc:\n",
    "        if e.is_stop == True:\n",
    "            pass\n",
    "        else:\n",
    "            lemm = e.lemma_ \n",
    "            lemmings.append(lemm)\n",
    "    scrubbed.append(lemmings)\n",
    "\n",
    "lems = pd.Series(scrubbed)\n",
    "df['lemmalist'] = lems.values\n",
    "ugrams = []\n",
    "for i in df['lemmalist'].astype(str).values:\n",
    "    i = re.sub(\",\", \"\", i)\n",
    "    i = re.sub(\"\\[\", \"\", i)\n",
    "    i = re.sub(\"\\]\", \"\", i)\n",
    "    i.strip()\n",
    "    i = ' '.join(filter(None,i.split(' ')))    \n",
    "    i = re.sub(\"'\", \"\", i)\n",
    "    ugrams.append(i)\n",
    "ugr = pd.Series(ugrams)    \n",
    "df['unigrams'] = ugr.values   \n",
    "df[['Title', 'corpus', 'lemmalist', 'unigrams', 'NERsMod']].head(3)\n",
    "\n",
    "#send to file\n",
    "#df.to_csv(\"C:\\\\datasources\\\\totsLEMS.csv\", sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate ngrams and their frequencies the old fashioned way\n",
    "utallies = []\n",
    "btallies = []\n",
    "ttallies = []\n",
    "utots = {}\n",
    "btots = {}\n",
    "ttots = {}\n",
    "c = cntr()\n",
    "for i in df['unigrams'].values:\n",
    "    utally = {}\n",
    "    btally = {}\n",
    "    ttally = {}\n",
    "    un = i.split()\n",
    "    bi = [' '.join(un[i:i+2]) for i in range(len(un)-2)]\n",
    "    tri = [' '.join(un[i:i+3]) for i in range(len(un)-3)]\n",
    "    for wrd in un:\n",
    "        if wrd not in utally:\n",
    "            utally[wrd] = 1            \n",
    "        else:\n",
    "            utally[wrd] += 1\n",
    "        if wrd not in utots:\n",
    "            utots[wrd] = 1\n",
    "        else:\n",
    "            utots[wrd] += 1\n",
    "    for bigram in bi:\n",
    "        if bigram not in btally:\n",
    "            btally[bigram] = 1\n",
    "            btots[bigram] = 1\n",
    "        else:\n",
    "            btally[bigram] += 1\n",
    "            btots[bigram] += 1\n",
    "    for trigram in tri:\n",
    "        if trigram not in ttally:\n",
    "            ttally[trigram] = 1\n",
    "            ttots[trigram] = 1\n",
    "        else:\n",
    "            ttally[trigram] += 1\n",
    "            ttots[trigram] += 1\n",
    "    ttallies.append(ttally)        \n",
    "    btallies.append(btally)\n",
    "    utallies.append(utally)\n",
    "    \n",
    "utal = pd.Series(utallies)\n",
    "btal = pd.Series(btallies)\n",
    "ttal = pd.Series(ttallies)\n",
    "df['UGC'] = utal.values\n",
    "df['BGC'] = btal.values\n",
    "df['TGC'] = ttal.values\n",
    "ugramTOTS = pd.DataFrame.from_dict(utots, orient='index')\n",
    "bigramTOTS = pd.DataFrame.from_dict(btots, orient='index')\n",
    "trigramTOTS = pd.DataFrame.from_dict(ttots, orient='index')\n",
    "\n",
    "#send to file\n",
    "#df.to_csv(\"C:\\\\datasources\\\\newsNLPready.csv\", sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entities identified by SpaCy need to be cleaned up and their frequencies determined\n",
    "allents = []\n",
    "allenttypes = []\n",
    "entcounts = {}\n",
    "\n",
    "for i in df['NERsMod'].astype(str).values:\n",
    "    testk = []\n",
    "    testv = []\n",
    "    i = re.sub(\", \", \" | \", i)\n",
    "    i = re.sub(\",\", \"\", i)\n",
    "    i = re.sub(\"{\", \"\", i)\n",
    "    i = re.sub(\"}\", \"\", i)\n",
    "    i = i.split(\" | \")\n",
    "    for d in i:\n",
    "        if d not in entcounts:\n",
    "            entcounts[d] = 1\n",
    "        else:\n",
    "            entcounts[d] += 1\n",
    "        d = d.split(\":\")\n",
    "        k = d[::2]\n",
    "        v = d[1::2]        \n",
    "        testk.append(k)    \n",
    "        testv.append(v)\n",
    "    allents.append(testk)\n",
    "    allenttypes.append(testv)\n",
    "\n",
    "tsk = pd.Series(allents)\n",
    "ets = pd.Series(allenttypes)\n",
    "\n",
    "df['EN'] = tsk.values\n",
    "df['ETYPES'] = ets.values\n",
    "\n",
    "new = []\n",
    "for it in df['EN'].astype(str).values:\n",
    "    it = re.sub(\"\\[\", \"\", it)\n",
    "    it = re.sub(\"\\]\", \"\", it)\n",
    "#    it = re.sub(\"\\'\", \"\", it)\n",
    "    it = re.sub(\"\\\"\", \"\", it)\n",
    "    new.append(it)\n",
    "\n",
    "tst = pd.Series(new)\n",
    "df['tst'] = tst.values     \n",
    "\n",
    "ENTdf = pd.DataFrame.from_dict(entcounts, orient='index')\n",
    "ENTdf.rename(columns={ ENTdf.columns[0]: \"Count\" }, inplace=True)\n",
    "ENTdf['raws'] = ENTdf.index\n",
    "ENTdf[['Entity', 'E_Type']] = ENTdf['raws'].str.split(':', expand=True)\n",
    "ENTdf['E_Type'] = ENTdf['E_Type'].astype(str).apply(lambda fx: fx.replace('\\'', ''))\n",
    "ENTdf['Entity'] = ENTdf['Entity'].astype(str).apply(lambda fx: fx.replace('\\'', ''))\n",
    "ENTdf['Entity'] = ENTdf['Entity'].astype(str).apply(lambda fx: fx.replace('this ', ''))\n",
    "ENTdf['Entity'] = ENTdf['Entity'].astype(str).apply(lambda fx: fx.replace('the ', ''))\n",
    "ENTdf['Entity'] = ENTdf['Entity'].astype(str).apply(lambda fx: fx.replace('The ', ''))\n",
    "ENTdf['Entity'] = ENTdf['Entity'].astype(str).apply(lambda fx: fx.replace('\\\"', ''))\n",
    "ENTdf.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "#save the entity types of interest and drop the rest\n",
    "droptype = ['CARDINAL', 'ORDINAL', 'DATE', 'LOC']\n",
    "#ENTdf = ENTdf[~ENTdf['E_Type'].isin(droptype)]\n",
    "ENTdf = ENTdf[~ENTdf['E_Type'].str.contains('|'.join(droptype))]\n",
    "ENTdf = ENTdf[~ENTdf['Entity'].str.contains('Nothing')]\n",
    "\n",
    "#find min max frequencies for best results and drop the rest\n",
    "focustype = ['PERSON', 'ORG']\n",
    "ENTdf = ENTdf[ENTdf['E_Type'].str.contains('|'.join(focustype))]\n",
    "ENTdf = ENTdf[ENTdf['Count'] > 4]\n",
    "ENTdf = ENTdf[ENTdf['Count'] < 74]\n",
    "\n",
    "#a bit more cleaning and save orgs to file\n",
    "orgDF = ENTdf[~ENTdf['E_Type'].str.contains('PERSON')]\n",
    "NAo = ['AP', 'CBS', 'UFO Information Center', 'Washington Post', 'Al Jazeera', 'ATM', 'AFP', 'Aftergood', 'Aftershock', 'mercola.com', 'banks', 'the state', 'cooperatives', 'pre911', 'post911', 'the state', 'Harvey', 'Capitol Hill', 'Fox News', 'ABC', 'Federal', 'Foundation', 'The Post', 'Standard Poors', 'Energy']\n",
    "orgDF = orgDF[~orgDF['Entity'].str.contains('|'.join(NAo))]\n",
    "orgDF.sort_values(by='Entity', inplace=True)\n",
    "\n",
    "orgDF[['Entity', 'Count']].to_csv(\"C:\\\\datasources\\\\identorgs.csv\", sep='|', encoding='utf-8')\n",
    "\n",
    "#a bit more cleaning and save people to file\n",
    "NAp = ['Explore', 'Spies', 'Lords', 'clients', 'kids', 'Anonymous', 'Armageddon', 'WiFi', 'Julia', 'Young', 'Black', 'Kerry', 'mm', 'deception10pg']\n",
    "pplDF = ENTdf[~ENTdf['E_Type'].str.contains('ORG')]\n",
    "pplDF = pplDF[~pplDF['Entity'].str.contains('|'.join(NAp))]\n",
    "pplDF.sort_values(by='Entity', inplace=True)\n",
    "\n",
    "pplDF[['Entity', 'Count']].to_csv(\"C:\\\\datasources\\\\identpeople.csv\", sep='|', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
